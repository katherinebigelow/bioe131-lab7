{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "## Part 1\n",
    "Using np.random.choice, generate 100 megabytes(8 bits/byte* 1024 bytes/kilobyte* 1024 kilobytes/megabyte* 100) of random data containing 100%, 90%, 80%, 70%, 60%, and 50% zeros. Be sure to call np.packbits on your data before writing it to a file. For example:\n",
    "\n",
    "`myvar= np.random.choice([0, 1], size=1024, replace=True, p=[0.5, 0.5])\n",
    "myvar = np.packbits(myvar)`\n",
    "\n",
    "Then write this data to a file in your home directory, like this:\n",
    "\n",
    "`open(“zeros_100p”, “wb”).write(zeros_100p)`\n",
    "\n",
    "Next, generate DNA and protein sequences100 millionletterslongand write those to your home directory.The probability of each letter should be equal.To write stringsgenerated in Numpy to a file, you’ll have to use a slightly different command, like this:\n",
    "\n",
    "`open(“nt_seq.fa”, “w”).write(“”.join(my_nt_seq))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "On each of thefiles you generated above, run gzip, bzip, pbzip2and ArithmeticCompress as follows:\n",
    "\n",
    "`time gzip –k zeros_100p\n",
    "time bzip2 –k zeros_100p\n",
    "time pbzip2 –k zeros_100p\n",
    "time ArithmeticCompresszeros_100p zeros_100p.art`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Which algorithm achieves the best level of compression on each file type?\n",
    "\n",
    "Which algorithm is the fastest?What is the difference between bzip2 andpbzip2? \n",
    "\n",
    "Do you expect one to be faster and why?How does the level of compression change as the percentage of zeros increases? Why does this happen?\n",
    "\n",
    "What is the minimum number of bits required to store a single DNA base?\n",
    "\n",
    "What is the minimum number of bits requiredto store an amino acid letter?\n",
    "\n",
    "In your tests, how many bits did gzip and bzip2 actually require to storeyour random DNA and protein sequences?\n",
    "\n",
    "Are gzip and bzip2 performing well on DNA and proteins?\n",
    "\n",
    "## Part 3\n",
    "Using what you’ve learned about querying biological databases, find the nucleic acid sequences of gp120 homologs from at least 10 differentHIV isolatesand concatenate them together into a single multi-FASTA.\n",
    "\n",
    "A priori, doyou expect to achieve better or worse compression here than random data? Why?\n",
    "\n",
    "Now, compress the multi-FASTAusing gzip, bzip2, and arithmetic coding. How does the compression ratio of this file compare to random data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "You’re working at a biotech company thatgenerates 1000 terabytes of data every day. In a meeting, your boss mentionsthat it costs the company \\$50 per terabyte of hard disk space, and so every 1% reduction in data that must be stored translates into a \\$500 savings per day.Your team will get a bonus this year equal to the amount of savingsyou’re able to generate by compressing the company’s data.\n",
    "\n",
    "Let’s make some assumptions about the contents of the data at your biotech company. Most of the data, say 80%, is re-sequencing of genomes and plasmids that are very similar to each other. Another 10% might be protein sequences, and the last 10% are binary microscope images which we’ll assume follow the worst-case scenario of being completely random. \n",
    "\n",
    "Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
